{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as learn\n",
    "import numpy as np\n",
    "from tensorflow.contrib.learn.python.learn.metric_spec import MetricSpec\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_l</th>\n",
       "      <th>sepal_w</th>\n",
       "      <th>petal_l</th>\n",
       "      <th>petal_w</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_l  sepal_w  petal_l  petal_w         species\n",
       "0        5.1      3.5      1.4      0.2     Iris-setosa\n",
       "1        4.9      3.0      1.4      0.2     Iris-setosa\n",
       "2        4.7      3.2      1.3      0.2     Iris-setosa\n",
       "3        4.6      3.1      1.5      0.2     Iris-setosa\n",
       "4        5.0      3.6      1.4      0.2     Iris-setosa\n",
       "5        5.4      3.9      1.7      0.4     Iris-setosa\n",
       "6        4.6      3.4      1.4      0.3     Iris-setosa\n",
       "7        5.0      3.4      1.5      0.2     Iris-setosa\n",
       "8        4.4      2.9      1.4      0.2     Iris-setosa\n",
       "9        4.9      3.1      1.5      0.1     Iris-setosa\n",
       "10       5.4      3.7      1.5      0.2     Iris-setosa\n",
       "11       4.8      3.4      1.6      0.2     Iris-setosa\n",
       "12       4.8      3.0      1.4      0.1     Iris-setosa\n",
       "13       4.3      3.0      1.1      0.1     Iris-setosa\n",
       "14       5.8      4.0      1.2      0.2     Iris-setosa\n",
       "15       5.7      4.4      1.5      0.4     Iris-setosa\n",
       "16       5.4      3.9      1.3      0.4     Iris-setosa\n",
       "17       5.1      3.5      1.4      0.3     Iris-setosa\n",
       "18       5.7      3.8      1.7      0.3     Iris-setosa\n",
       "19       5.1      3.8      1.5      0.3     Iris-setosa\n",
       "20       5.4      3.4      1.7      0.2     Iris-setosa\n",
       "21       5.1      3.7      1.5      0.4     Iris-setosa\n",
       "22       4.6      3.6      1.0      0.2     Iris-setosa\n",
       "23       5.1      3.3      1.7      0.5     Iris-setosa\n",
       "24       4.8      3.4      1.9      0.2     Iris-setosa\n",
       "25       5.0      3.0      1.6      0.2     Iris-setosa\n",
       "26       5.0      3.4      1.6      0.4     Iris-setosa\n",
       "27       5.2      3.5      1.5      0.2     Iris-setosa\n",
       "28       5.2      3.4      1.4      0.2     Iris-setosa\n",
       "29       4.7      3.2      1.6      0.2     Iris-setosa\n",
       "..       ...      ...      ...      ...             ...\n",
       "120      6.9      3.2      5.7      2.3  Iris-virginica\n",
       "121      5.6      2.8      4.9      2.0  Iris-virginica\n",
       "122      7.7      2.8      6.7      2.0  Iris-virginica\n",
       "123      6.3      2.7      4.9      1.8  Iris-virginica\n",
       "124      6.7      3.3      5.7      2.1  Iris-virginica\n",
       "125      7.2      3.2      6.0      1.8  Iris-virginica\n",
       "126      6.2      2.8      4.8      1.8  Iris-virginica\n",
       "127      6.1      3.0      4.9      1.8  Iris-virginica\n",
       "128      6.4      2.8      5.6      2.1  Iris-virginica\n",
       "129      7.2      3.0      5.8      1.6  Iris-virginica\n",
       "130      7.4      2.8      6.1      1.9  Iris-virginica\n",
       "131      7.9      3.8      6.4      2.0  Iris-virginica\n",
       "132      6.4      2.8      5.6      2.2  Iris-virginica\n",
       "133      6.3      2.8      5.1      1.5  Iris-virginica\n",
       "134      6.1      2.6      5.6      1.4  Iris-virginica\n",
       "135      7.7      3.0      6.1      2.3  Iris-virginica\n",
       "136      6.3      3.4      5.6      2.4  Iris-virginica\n",
       "137      6.4      3.1      5.5      1.8  Iris-virginica\n",
       "138      6.0      3.0      4.8      1.8  Iris-virginica\n",
       "139      6.9      3.1      5.4      2.1  Iris-virginica\n",
       "140      6.7      3.1      5.6      2.4  Iris-virginica\n",
       "141      6.9      3.1      5.1      2.3  Iris-virginica\n",
       "142      5.8      2.7      5.1      1.9  Iris-virginica\n",
       "143      6.8      3.2      5.9      2.3  Iris-virginica\n",
       "144      6.7      3.3      5.7      2.5  Iris-virginica\n",
       "145      6.7      3.0      5.2      2.3  Iris-virginica\n",
       "146      6.3      2.5      5.0      1.9  Iris-virginica\n",
       "147      6.5      3.0      5.2      2.0  Iris-virginica\n",
       "148      6.2      3.4      5.4      2.3  Iris-virginica\n",
       "149      5.9      3.0      5.1      1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the desired TensorFlow output level\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "path = \"./data/\"\n",
    "    \n",
    "filename = os.path.join(path,\"iris.csv\")    \n",
    "df = pd.read_csv(filename,na_values=['NA','?'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_l</th>\n",
       "      <th>sepal_w</th>\n",
       "      <th>petal_l</th>\n",
       "      <th>petal_w</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.897674</td>\n",
       "      <td>1.015602</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.139200</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.380727</td>\n",
       "      <td>0.327318</td>\n",
       "      <td>-1.392399</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.501490</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.018437</td>\n",
       "      <td>1.245030</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.535384</td>\n",
       "      <td>1.933315</td>\n",
       "      <td>-1.165809</td>\n",
       "      <td>-1.048667</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.501490</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.179859</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.018437</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.743017</td>\n",
       "      <td>-0.360967</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.139200</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.442245</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.535384</td>\n",
       "      <td>1.474458</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.259964</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.222456</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.259964</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.442245</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.863780</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>-1.505695</td>\n",
       "      <td>-1.442245</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.052331</td>\n",
       "      <td>2.162743</td>\n",
       "      <td>-1.449047</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.173094</td>\n",
       "      <td>3.080455</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.048667</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.535384</td>\n",
       "      <td>1.933315</td>\n",
       "      <td>-1.392399</td>\n",
       "      <td>-1.048667</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.897674</td>\n",
       "      <td>1.015602</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.179859</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.173094</td>\n",
       "      <td>1.703886</td>\n",
       "      <td>-1.165809</td>\n",
       "      <td>-1.179859</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.897674</td>\n",
       "      <td>1.703886</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.179859</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.535384</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.165809</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.897674</td>\n",
       "      <td>1.474458</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.048667</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.501490</td>\n",
       "      <td>1.245030</td>\n",
       "      <td>-1.562342</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.897674</td>\n",
       "      <td>0.556746</td>\n",
       "      <td>-1.165809</td>\n",
       "      <td>-0.917474</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.259964</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.052513</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.018437</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>-1.222456</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.018437</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.222456</td>\n",
       "      <td>-1.048667</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.776911</td>\n",
       "      <td>1.015602</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.776911</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.380727</td>\n",
       "      <td>0.327318</td>\n",
       "      <td>-1.222456</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1.276066</td>\n",
       "      <td>0.327318</td>\n",
       "      <td>1.100097</td>\n",
       "      <td>1.443994</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-0.293857</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>0.646916</td>\n",
       "      <td>1.050416</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2.242172</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>1.666574</td>\n",
       "      <td>1.050416</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.551486</td>\n",
       "      <td>-0.819823</td>\n",
       "      <td>0.646916</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1.034539</td>\n",
       "      <td>0.556746</td>\n",
       "      <td>1.100097</td>\n",
       "      <td>1.181609</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1.638355</td>\n",
       "      <td>0.327318</td>\n",
       "      <td>1.270040</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.430722</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>0.590269</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.309959</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>0.646916</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.672249</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>1.043450</td>\n",
       "      <td>1.181609</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1.638355</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>1.156745</td>\n",
       "      <td>0.525645</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1.879882</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>1.326688</td>\n",
       "      <td>0.919223</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2.483699</td>\n",
       "      <td>1.703886</td>\n",
       "      <td>1.496631</td>\n",
       "      <td>1.050416</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.672249</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>1.043450</td>\n",
       "      <td>1.312801</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.551486</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>0.760211</td>\n",
       "      <td>0.394453</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.309959</td>\n",
       "      <td>-1.049251</td>\n",
       "      <td>1.043450</td>\n",
       "      <td>0.263260</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2.242172</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>1.326688</td>\n",
       "      <td>1.443994</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.551486</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>1.043450</td>\n",
       "      <td>1.575187</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.672249</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>0.986802</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.189196</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>0.590269</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1.276066</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>0.930154</td>\n",
       "      <td>1.181609</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1.034539</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>1.043450</td>\n",
       "      <td>1.575187</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1.276066</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>0.760211</td>\n",
       "      <td>1.443994</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-0.052331</td>\n",
       "      <td>-0.819823</td>\n",
       "      <td>0.760211</td>\n",
       "      <td>0.919223</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1.155302</td>\n",
       "      <td>0.327318</td>\n",
       "      <td>1.213393</td>\n",
       "      <td>1.443994</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1.034539</td>\n",
       "      <td>0.556746</td>\n",
       "      <td>1.100097</td>\n",
       "      <td>1.706379</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.034539</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>0.816859</td>\n",
       "      <td>1.443994</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.551486</td>\n",
       "      <td>-1.278680</td>\n",
       "      <td>0.703564</td>\n",
       "      <td>0.919223</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.793012</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>0.816859</td>\n",
       "      <td>1.050416</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.430722</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>0.930154</td>\n",
       "      <td>1.443994</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.068433</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>0.760211</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sepal_l   sepal_w   petal_l   petal_w         species\n",
       "0   -0.897674  1.015602 -1.335752 -1.311052     Iris-setosa\n",
       "1   -1.139200 -0.131539 -1.335752 -1.311052     Iris-setosa\n",
       "2   -1.380727  0.327318 -1.392399 -1.311052     Iris-setosa\n",
       "3   -1.501490  0.097889 -1.279104 -1.311052     Iris-setosa\n",
       "4   -1.018437  1.245030 -1.335752 -1.311052     Iris-setosa\n",
       "5   -0.535384  1.933315 -1.165809 -1.048667     Iris-setosa\n",
       "6   -1.501490  0.786174 -1.335752 -1.179859     Iris-setosa\n",
       "7   -1.018437  0.786174 -1.279104 -1.311052     Iris-setosa\n",
       "8   -1.743017 -0.360967 -1.335752 -1.311052     Iris-setosa\n",
       "9   -1.139200  0.097889 -1.279104 -1.442245     Iris-setosa\n",
       "10  -0.535384  1.474458 -1.279104 -1.311052     Iris-setosa\n",
       "11  -1.259964  0.786174 -1.222456 -1.311052     Iris-setosa\n",
       "12  -1.259964 -0.131539 -1.335752 -1.442245     Iris-setosa\n",
       "13  -1.863780 -0.131539 -1.505695 -1.442245     Iris-setosa\n",
       "14  -0.052331  2.162743 -1.449047 -1.311052     Iris-setosa\n",
       "15  -0.173094  3.080455 -1.279104 -1.048667     Iris-setosa\n",
       "16  -0.535384  1.933315 -1.392399 -1.048667     Iris-setosa\n",
       "17  -0.897674  1.015602 -1.335752 -1.179859     Iris-setosa\n",
       "18  -0.173094  1.703886 -1.165809 -1.179859     Iris-setosa\n",
       "19  -0.897674  1.703886 -1.279104 -1.179859     Iris-setosa\n",
       "20  -0.535384  0.786174 -1.165809 -1.311052     Iris-setosa\n",
       "21  -0.897674  1.474458 -1.279104 -1.048667     Iris-setosa\n",
       "22  -1.501490  1.245030 -1.562342 -1.311052     Iris-setosa\n",
       "23  -0.897674  0.556746 -1.165809 -0.917474     Iris-setosa\n",
       "24  -1.259964  0.786174 -1.052513 -1.311052     Iris-setosa\n",
       "25  -1.018437 -0.131539 -1.222456 -1.311052     Iris-setosa\n",
       "26  -1.018437  0.786174 -1.222456 -1.048667     Iris-setosa\n",
       "27  -0.776911  1.015602 -1.279104 -1.311052     Iris-setosa\n",
       "28  -0.776911  0.786174 -1.335752 -1.311052     Iris-setosa\n",
       "29  -1.380727  0.327318 -1.222456 -1.311052     Iris-setosa\n",
       "..        ...       ...       ...       ...             ...\n",
       "120  1.276066  0.327318  1.100097  1.443994  Iris-virginica\n",
       "121 -0.293857 -0.590395  0.646916  1.050416  Iris-virginica\n",
       "122  2.242172 -0.590395  1.666574  1.050416  Iris-virginica\n",
       "123  0.551486 -0.819823  0.646916  0.788031  Iris-virginica\n",
       "124  1.034539  0.556746  1.100097  1.181609  Iris-virginica\n",
       "125  1.638355  0.327318  1.270040  0.788031  Iris-virginica\n",
       "126  0.430722 -0.590395  0.590269  0.788031  Iris-virginica\n",
       "127  0.309959 -0.131539  0.646916  0.788031  Iris-virginica\n",
       "128  0.672249 -0.590395  1.043450  1.181609  Iris-virginica\n",
       "129  1.638355 -0.131539  1.156745  0.525645  Iris-virginica\n",
       "130  1.879882 -0.590395  1.326688  0.919223  Iris-virginica\n",
       "131  2.483699  1.703886  1.496631  1.050416  Iris-virginica\n",
       "132  0.672249 -0.590395  1.043450  1.312801  Iris-virginica\n",
       "133  0.551486 -0.590395  0.760211  0.394453  Iris-virginica\n",
       "134  0.309959 -1.049251  1.043450  0.263260  Iris-virginica\n",
       "135  2.242172 -0.131539  1.326688  1.443994  Iris-virginica\n",
       "136  0.551486  0.786174  1.043450  1.575187  Iris-virginica\n",
       "137  0.672249  0.097889  0.986802  0.788031  Iris-virginica\n",
       "138  0.189196 -0.131539  0.590269  0.788031  Iris-virginica\n",
       "139  1.276066  0.097889  0.930154  1.181609  Iris-virginica\n",
       "140  1.034539  0.097889  1.043450  1.575187  Iris-virginica\n",
       "141  1.276066  0.097889  0.760211  1.443994  Iris-virginica\n",
       "142 -0.052331 -0.819823  0.760211  0.919223  Iris-virginica\n",
       "143  1.155302  0.327318  1.213393  1.443994  Iris-virginica\n",
       "144  1.034539  0.556746  1.100097  1.706379  Iris-virginica\n",
       "145  1.034539 -0.131539  0.816859  1.443994  Iris-virginica\n",
       "146  0.551486 -1.278680  0.703564  0.919223  Iris-virginica\n",
       "147  0.793012 -0.131539  0.816859  1.050416  Iris-virginica\n",
       "148  0.430722  0.786174  0.930154  1.443994  Iris-virginica\n",
       "149  0.068433 -0.131539  0.760211  0.788031  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding a numeric column as zscores\n",
    "def encode_numeric_zscore(df,name,mean=None,sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name]-mean)/sd\n",
    "\n",
    "# Encoding feature vector\n",
    "encode_numeric_zscore(df,'petal_w')\n",
    "encode_numeric_zscore(df,'petal_l')\n",
    "encode_numeric_zscore(df,'sepal_w')\n",
    "encode_numeric_zscore(df,'sepal_l') \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_l</th>\n",
       "      <th>sepal_w</th>\n",
       "      <th>petal_l</th>\n",
       "      <th>petal_w</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.897674</td>\n",
       "      <td>1.015602</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.139200</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.380727</td>\n",
       "      <td>0.327318</td>\n",
       "      <td>-1.392399</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.501490</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.018437</td>\n",
       "      <td>1.245030</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.535384</td>\n",
       "      <td>1.933315</td>\n",
       "      <td>-1.165809</td>\n",
       "      <td>-1.048667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.501490</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.179859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.018437</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.743017</td>\n",
       "      <td>-0.360967</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.139200</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.442245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.535384</td>\n",
       "      <td>1.474458</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.259964</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.222456</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.259964</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.442245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.863780</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>-1.505695</td>\n",
       "      <td>-1.442245</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.052331</td>\n",
       "      <td>2.162743</td>\n",
       "      <td>-1.449047</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.173094</td>\n",
       "      <td>3.080455</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.048667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.535384</td>\n",
       "      <td>1.933315</td>\n",
       "      <td>-1.392399</td>\n",
       "      <td>-1.048667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.897674</td>\n",
       "      <td>1.015602</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.179859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.173094</td>\n",
       "      <td>1.703886</td>\n",
       "      <td>-1.165809</td>\n",
       "      <td>-1.179859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.897674</td>\n",
       "      <td>1.703886</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.179859</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.535384</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.165809</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.897674</td>\n",
       "      <td>1.474458</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.048667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.501490</td>\n",
       "      <td>1.245030</td>\n",
       "      <td>-1.562342</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.897674</td>\n",
       "      <td>0.556746</td>\n",
       "      <td>-1.165809</td>\n",
       "      <td>-0.917474</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.259964</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.052513</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.018437</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>-1.222456</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.018437</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.222456</td>\n",
       "      <td>-1.048667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.776911</td>\n",
       "      <td>1.015602</td>\n",
       "      <td>-1.279104</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.776911</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>-1.335752</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.380727</td>\n",
       "      <td>0.327318</td>\n",
       "      <td>-1.222456</td>\n",
       "      <td>-1.311052</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1.276066</td>\n",
       "      <td>0.327318</td>\n",
       "      <td>1.100097</td>\n",
       "      <td>1.443994</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-0.293857</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>0.646916</td>\n",
       "      <td>1.050416</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>2.242172</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>1.666574</td>\n",
       "      <td>1.050416</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.551486</td>\n",
       "      <td>-0.819823</td>\n",
       "      <td>0.646916</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1.034539</td>\n",
       "      <td>0.556746</td>\n",
       "      <td>1.100097</td>\n",
       "      <td>1.181609</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1.638355</td>\n",
       "      <td>0.327318</td>\n",
       "      <td>1.270040</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.430722</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>0.590269</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.309959</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>0.646916</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.672249</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>1.043450</td>\n",
       "      <td>1.181609</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1.638355</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>1.156745</td>\n",
       "      <td>0.525645</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1.879882</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>1.326688</td>\n",
       "      <td>0.919223</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>2.483699</td>\n",
       "      <td>1.703886</td>\n",
       "      <td>1.496631</td>\n",
       "      <td>1.050416</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.672249</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>1.043450</td>\n",
       "      <td>1.312801</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>0.551486</td>\n",
       "      <td>-0.590395</td>\n",
       "      <td>0.760211</td>\n",
       "      <td>0.394453</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>0.309959</td>\n",
       "      <td>-1.049251</td>\n",
       "      <td>1.043450</td>\n",
       "      <td>0.263260</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2.242172</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>1.326688</td>\n",
       "      <td>1.443994</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>0.551486</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>1.043450</td>\n",
       "      <td>1.575187</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>0.672249</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>0.986802</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.189196</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>0.590269</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1.276066</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>0.930154</td>\n",
       "      <td>1.181609</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1.034539</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>1.043450</td>\n",
       "      <td>1.575187</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>1.276066</td>\n",
       "      <td>0.097889</td>\n",
       "      <td>0.760211</td>\n",
       "      <td>1.443994</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-0.052331</td>\n",
       "      <td>-0.819823</td>\n",
       "      <td>0.760211</td>\n",
       "      <td>0.919223</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1.155302</td>\n",
       "      <td>0.327318</td>\n",
       "      <td>1.213393</td>\n",
       "      <td>1.443994</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>1.034539</td>\n",
       "      <td>0.556746</td>\n",
       "      <td>1.100097</td>\n",
       "      <td>1.706379</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1.034539</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>0.816859</td>\n",
       "      <td>1.443994</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.551486</td>\n",
       "      <td>-1.278680</td>\n",
       "      <td>0.703564</td>\n",
       "      <td>0.919223</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.793012</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>0.816859</td>\n",
       "      <td>1.050416</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.430722</td>\n",
       "      <td>0.786174</td>\n",
       "      <td>0.930154</td>\n",
       "      <td>1.443994</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.068433</td>\n",
       "      <td>-0.131539</td>\n",
       "      <td>0.760211</td>\n",
       "      <td>0.788031</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sepal_l   sepal_w   petal_l   petal_w  species\n",
       "0   -0.897674  1.015602 -1.335752 -1.311052        0\n",
       "1   -1.139200 -0.131539 -1.335752 -1.311052        0\n",
       "2   -1.380727  0.327318 -1.392399 -1.311052        0\n",
       "3   -1.501490  0.097889 -1.279104 -1.311052        0\n",
       "4   -1.018437  1.245030 -1.335752 -1.311052        0\n",
       "5   -0.535384  1.933315 -1.165809 -1.048667        0\n",
       "6   -1.501490  0.786174 -1.335752 -1.179859        0\n",
       "7   -1.018437  0.786174 -1.279104 -1.311052        0\n",
       "8   -1.743017 -0.360967 -1.335752 -1.311052        0\n",
       "9   -1.139200  0.097889 -1.279104 -1.442245        0\n",
       "10  -0.535384  1.474458 -1.279104 -1.311052        0\n",
       "11  -1.259964  0.786174 -1.222456 -1.311052        0\n",
       "12  -1.259964 -0.131539 -1.335752 -1.442245        0\n",
       "13  -1.863780 -0.131539 -1.505695 -1.442245        0\n",
       "14  -0.052331  2.162743 -1.449047 -1.311052        0\n",
       "15  -0.173094  3.080455 -1.279104 -1.048667        0\n",
       "16  -0.535384  1.933315 -1.392399 -1.048667        0\n",
       "17  -0.897674  1.015602 -1.335752 -1.179859        0\n",
       "18  -0.173094  1.703886 -1.165809 -1.179859        0\n",
       "19  -0.897674  1.703886 -1.279104 -1.179859        0\n",
       "20  -0.535384  0.786174 -1.165809 -1.311052        0\n",
       "21  -0.897674  1.474458 -1.279104 -1.048667        0\n",
       "22  -1.501490  1.245030 -1.562342 -1.311052        0\n",
       "23  -0.897674  0.556746 -1.165809 -0.917474        0\n",
       "24  -1.259964  0.786174 -1.052513 -1.311052        0\n",
       "25  -1.018437 -0.131539 -1.222456 -1.311052        0\n",
       "26  -1.018437  0.786174 -1.222456 -1.048667        0\n",
       "27  -0.776911  1.015602 -1.279104 -1.311052        0\n",
       "28  -0.776911  0.786174 -1.335752 -1.311052        0\n",
       "29  -1.380727  0.327318 -1.222456 -1.311052        0\n",
       "..        ...       ...       ...       ...      ...\n",
       "120  1.276066  0.327318  1.100097  1.443994        2\n",
       "121 -0.293857 -0.590395  0.646916  1.050416        2\n",
       "122  2.242172 -0.590395  1.666574  1.050416        2\n",
       "123  0.551486 -0.819823  0.646916  0.788031        2\n",
       "124  1.034539  0.556746  1.100097  1.181609        2\n",
       "125  1.638355  0.327318  1.270040  0.788031        2\n",
       "126  0.430722 -0.590395  0.590269  0.788031        2\n",
       "127  0.309959 -0.131539  0.646916  0.788031        2\n",
       "128  0.672249 -0.590395  1.043450  1.181609        2\n",
       "129  1.638355 -0.131539  1.156745  0.525645        2\n",
       "130  1.879882 -0.590395  1.326688  0.919223        2\n",
       "131  2.483699  1.703886  1.496631  1.050416        2\n",
       "132  0.672249 -0.590395  1.043450  1.312801        2\n",
       "133  0.551486 -0.590395  0.760211  0.394453        2\n",
       "134  0.309959 -1.049251  1.043450  0.263260        2\n",
       "135  2.242172 -0.131539  1.326688  1.443994        2\n",
       "136  0.551486  0.786174  1.043450  1.575187        2\n",
       "137  0.672249  0.097889  0.986802  0.788031        2\n",
       "138  0.189196 -0.131539  0.590269  0.788031        2\n",
       "139  1.276066  0.097889  0.930154  1.181609        2\n",
       "140  1.034539  0.097889  1.043450  1.575187        2\n",
       "141  1.276066  0.097889  0.760211  1.443994        2\n",
       "142 -0.052331 -0.819823  0.760211  0.919223        2\n",
       "143  1.155302  0.327318  1.213393  1.443994        2\n",
       "144  1.034539  0.556746  1.100097  1.706379        2\n",
       "145  1.034539 -0.131539  0.816859  1.443994        2\n",
       "146  0.551486 -1.278680  0.703564  0.919223        2\n",
       "147  0.793012 -0.131539  0.816859  1.050416        2\n",
       "148  0.430722  0.786174  0.930154  1.443994        2\n",
       "149  0.068433 -0.131539  0.760211  0.788031        2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encodun text values to indexes(i.e. [1],[2],[3] for red,green,blue).\n",
    "def encode_text_index(df,name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "species = encode_text_index(df,\"species\")\n",
    "num_classes = len(species)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert a Pandas dataframe to the x,y inputs that TensorFlow needs\n",
    "def to_xy(df,target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "\n",
    "    # find out the type of the target column.\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    \n",
    "    # Encode to int for classification, float otherwise.\n",
    "    if target_type in (np.int64, np.int32):\n",
    "        # Classification\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.int32)\n",
    "    else:\n",
    "        # Regression\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.float32)\n",
    "\n",
    "\n",
    "# Create the x | Y -side  of the training\n",
    "x, y = to_xy(df,'species')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split into train/test\n",
    "x_train, x_test, y_train, y_test = train_test_split(    \n",
    "    x, y, test_size=0.25, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model_dir(name,erase):\n",
    "    base_path = os.path.join(\".\",\"dnn\")\n",
    "    model_dir = os.path.join(base_path,name)\n",
    "    os.makedirs(model_dir,exist_ok=True)\n",
    "    if erase and len(model_dir)>4 and os.path.isdir(model_dir):\n",
    "        shutil.rmtree(model_dir,ignore_errors=True) # be careful, this deletes everything below the specified path\n",
    "    return model_dir\n",
    "\n",
    "# Get/clear a directory to store the neural network to\n",
    "model_dir = get_model_dir('iris',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'keep_checkpoint_every_n_hours': 10000, '_evaluation_master': '', 'tf_random_seed': None, 'save_summary_steps': 100, '_environment': 'local', '_master': '', 'save_checkpoints_secs': 1, '_task_id': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4963429f60>, '_task_type': None, 'keep_checkpoint_max': 5, 'tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_ps_replicas': 0, 'save_checkpoints_steps': None}\n"
     ]
    }
   ],
   "source": [
    "# Create a deep neural network with 3 hidden layers of 10, 20, 5\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "classifier = learn.DNNClassifier(\n",
    "    model_dir= model_dir,\n",
    "    config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1),\n",
    "    hidden_units=[10, 20, 5], n_classes=num_classes, feature_columns=feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/irfanalidv/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:322 in __init__.: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n"
     ]
    }
   ],
   "source": [
    "# Early stopping\n",
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    every_n_steps=500,\n",
    "    #metrics=validation_metrics,\n",
    "    early_stopping_metric=\"loss\",\n",
    "    early_stopping_metric_minimize=True,\n",
    "    early_stopping_rounds=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/irfanalidv/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:315 in fit.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /home/irfanalidv/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:315 in fit.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /home/irfanalidv/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:315 in fit.: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:fraction_of_zero_values is illegal; using dnn/hiddenlayer_0_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:activation is illegal; using dnn/hiddenlayer_0_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:fraction_of_zero_values is illegal; using dnn/hiddenlayer_1_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:activation is illegal; using dnn/hiddenlayer_1_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:fraction_of_zero_values is illegal; using dnn/hiddenlayer_2_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:activation is illegal; using dnn/hiddenlayer_2_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/logits:fraction_of_zero_values is illegal; using dnn/logits_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/logits:activation is illegal; using dnn/logits_activation instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:loss = 1.07145, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./dnn/iris/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:loss = 0.218818, step = 101\n",
      "INFO:tensorflow:global_step/sec: 374.007\n",
      "INFO:tensorflow:loss = 0.0897297, step = 201\n",
      "INFO:tensorflow:global_step/sec: 632.789\n",
      "INFO:tensorflow:loss = 0.0482916, step = 301\n",
      "INFO:tensorflow:global_step/sec: 470.312\n",
      "INFO:tensorflow:loss = 0.0329476, step = 401\n",
      "INFO:tensorflow:global_step/sec: 443.433\n",
      "INFO:tensorflow:Saving checkpoints for 465 into ./dnn/iris/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:From /home/irfanalidv/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /home/irfanalidv/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /home/irfanalidv/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:fraction_of_zero_values is illegal; using dnn/hiddenlayer_0_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:activation is illegal; using dnn/hiddenlayer_0_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:fraction_of_zero_values is illegal; using dnn/hiddenlayer_1_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:activation is illegal; using dnn/hiddenlayer_1_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:fraction_of_zero_values is illegal; using dnn/hiddenlayer_2_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:activation is illegal; using dnn/hiddenlayer_2_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/logits:fraction_of_zero_values is illegal; using dnn/logits_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/logits:activation is illegal; using dnn/logits_activation instead.\n",
      "INFO:tensorflow:Restored model from ./dnn/iris\n",
      "INFO:tensorflow:Eval steps [0,inf) for training step 465.\n",
      "INFO:tensorflow:Input iterator is exhausted.\n",
      "INFO:tensorflow:Saving evaluation summary for step 465: accuracy = 0.973684, loss = 0.107866\n",
      "INFO:tensorflow:Validation (step 500): global_step = 465, accuracy = 0.973684, loss = 0.107866\n",
      "INFO:tensorflow:loss = 0.0252294, step = 501\n",
      "INFO:tensorflow:global_step/sec: 123.927\n",
      "INFO:tensorflow:loss = 0.0204662, step = 601\n",
      "INFO:tensorflow:global_step/sec: 524.815\n",
      "INFO:tensorflow:Saving checkpoints for 679 into ./dnn/iris/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:loss = 0.0171362, step = 701\n",
      "INFO:tensorflow:global_step/sec: 443.596\n",
      "INFO:tensorflow:loss = 0.0145132, step = 801\n",
      "INFO:tensorflow:global_step/sec: 527.395\n",
      "INFO:tensorflow:loss = 0.0123654, step = 901\n",
      "INFO:tensorflow:global_step/sec: 526.086\n",
      "WARNING:tensorflow:From /home/irfanalidv/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /home/irfanalidv/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From /home/irfanalidv/anaconda2/envs/py35/lib/python3.5/site-packages/tensorflow/contrib/learn/python/learn/estimators/dnn.py:323 in evaluate.: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:fraction_of_zero_values is illegal; using dnn/hiddenlayer_0_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_0:activation is illegal; using dnn/hiddenlayer_0_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:fraction_of_zero_values is illegal; using dnn/hiddenlayer_1_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_1:activation is illegal; using dnn/hiddenlayer_1_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:fraction_of_zero_values is illegal; using dnn/hiddenlayer_2_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/hiddenlayer_2:activation is illegal; using dnn/hiddenlayer_2_activation instead.\n",
      "INFO:tensorflow:Summary name dnn/logits:fraction_of_zero_values is illegal; using dnn/logits_fraction_of_zero_values instead.\n",
      "INFO:tensorflow:Summary name dnn/logits:activation is illegal; using dnn/logits_activation instead.\n",
      "INFO:tensorflow:Restored model from ./dnn/iris\n",
      "INFO:tensorflow:Eval steps [0,inf) for training step 679.\n",
      "INFO:tensorflow:Input iterator is exhausted.\n",
      "INFO:tensorflow:Saving evaluation summary for step 679: accuracy = 0.973684, loss = 0.129121\n",
      "INFO:tensorflow:Validation (step 1000): global_step = 679, accuracy = 0.973684, loss = 0.129121\n",
      "INFO:tensorflow:Stopping. Best step: 500 with loss = 0.1078663095831871.\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into ./dnn/iris/model.ckpt.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "WARNING:tensorflow:TensorFlow's V1 checkpoint format has been deprecated.\n",
      "WARNING:tensorflow:Consider switching to the more efficient V2 format:\n",
      "WARNING:tensorflow:   `tf.train.Saver(write_version=tf.train.SaverDef.V2)`\n",
      "WARNING:tensorflow:now on by default.\n",
      "WARNING:tensorflow:*******************************************************\n",
      "INFO:tensorflow:Loss for final step: 0.0104561.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.contrib.learn.python.learn.estimators.dnn.DNNClassifier at 0x7f4963429f98>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit/train neural network\n",
    "classifier.fit(x_train, y_train,monitors=[validation_monitor],steps=10000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Set the desired TensorFlow output level \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# Evaluate success using accuracy\n",
    "pred = list(classifier.predict(x_test, as_iterable=True))\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "print(\"Accuracy score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy array of predictions\n",
      "[ 99.9995   0.       0.0005]\n",
      "\n",
      "\n",
      "Log loss score: 0.16742906725971882\n"
     ]
    }
   ],
   "source": [
    "# Set the desired TensorFlow output\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# Generate predictions\n",
    "pred = list(classifier.predict_proba(x_test, as_iterable=True))\n",
    "\n",
    "print(\"Numpy array of predictions\")\n",
    "print(pred[0]*100)\n",
    "print(\"\\n\")\n",
    "\n",
    "#print(\"As percent probability\")\n",
    "#display(pred[0:5])\n",
    "\n",
    "score = metrics.log_loss(y_test, pred)\n",
    "print(\"Log loss score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAE0CAYAAAASSJRcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noA0AAAAUAUEbAAAAKIL/D5Cugg/MbciLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f494a637e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import figure, show\n",
    "from numpy import arange, sin, pi\n",
    "\n",
    "t = arange(1e-5, 5.0, 0.00001)\n",
    "#t = arange(1.0, 5.0, 0.00001) # computer scientists\n",
    "#t = arange(0.0, 1.0, 0.00001)  # data     scientists\n",
    "\n",
    "fig = figure(1,figsize=(12, 10))\n",
    "\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.plot(t, np.log(t))\n",
    "ax1.grid(True)\n",
    "ax1.set_ylim((-8, 1.5))\n",
    "ax1.set_xlim((-0.1, 2))\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('log(x)')\n",
    "\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
